{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xc_YUjThoBSv"
      },
      "outputs": [],
      "source": [
        "!pip install torch xgboost numpy pandas scikit-learn matplotlib\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n"
      ],
      "metadata": {
        "id": "Nr2ss-ogoQVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "T = 1500\n",
        "time = np.arange(T)\n",
        "\n",
        "trend = 0.01 * time\n",
        "seasonality = 10 * np.sin(2 * np.pi * time / 50)\n",
        "\n",
        "regime = np.where(time > 800, 20, 0)\n",
        "\n",
        "noise = np.random.normal(0, 1 + 0.005*time, T)\n",
        "\n",
        "y = trend + seasonality + regime + noise\n",
        "\n",
        "data = pd.DataFrame({\n",
        "    \"f1\": trend,\n",
        "    \"f2\": seasonality,\n",
        "    \"f3\": regime,\n",
        "    \"y\": y\n",
        "})\n",
        "\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.plot(y)\n",
        "plt.title(\"Synthetic Time Series\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "LsAt4nGXoWon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_sequences(data, seq_len=30):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - seq_len):\n",
        "        X.append(data.iloc[i:i+seq_len][[\"f1\",\"f2\",\"f3\"]].values)\n",
        "        y.append(data.iloc[i+seq_len][\"y\"])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "X, y = create_sequences(data, seq_len=30)\n"
      ],
      "metadata": {
        "id": "SqyhSapWopuH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler_X = StandardScaler()\n",
        "scaler_y = StandardScaler()\n",
        "\n",
        "X_flat = X.reshape(-1, X.shape[-1])\n",
        "X_scaled = scaler_X.fit_transform(X_flat).reshape(X.shape)\n",
        "\n",
        "y_scaled = scaler_y.fit_transform(y.reshape(-1,1)).flatten()\n"
      ],
      "metadata": {
        "id": "ftvb2QSPox2f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, input_dim=3, d_model=64, nhead=4, num_layers=2):\n",
        "        super().__init__()\n",
        "\n",
        "        self.input_projection = nn.Linear(input_dim, d_model)\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=nhead,\n",
        "            dim_feedforward=128,\n",
        "            dropout=0.1,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "        self.fc = nn.Linear(d_model, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.input_projection(x)\n",
        "        x = self.transformer(x)\n",
        "        x = x.mean(dim=1)\n",
        "        return self.fc(x)\n"
      ],
      "metadata": {
        "id": "fNYT8vH0o344"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rolling_origin_cv_transformer(X, y, n_splits=5, train_size=800, step=100):\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for i in range(n_splits):\n",
        "\n",
        "        end_train = train_size + i * step\n",
        "        end_test = end_train + step\n",
        "\n",
        "        X_train = torch.tensor(X[:end_train], dtype=torch.float32)\n",
        "        y_train = torch.tensor(y[:end_train], dtype=torch.float32).view(-1,1)\n",
        "\n",
        "        X_test = torch.tensor(X[end_train:end_test], dtype=torch.float32)\n",
        "        y_test = y[end_train:end_test]\n",
        "\n",
        "        model = TransformerModel()\n",
        "        criterion = nn.MSELoss()\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "        for epoch in range(20):\n",
        "            model.train()\n",
        "            optimizer.zero_grad()\n",
        "            output = model(X_train)\n",
        "            loss = criterion(output, y_train)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            preds = model(X_test).numpy()\n",
        "\n",
        "        preds = scaler_y.inverse_transform(preds)\n",
        "        y_true = scaler_y.inverse_transform(y_test.reshape(-1,1))\n",
        "\n",
        "        mae = mean_absolute_error(y_true, preds)\n",
        "        rmse = np.sqrt(mean_squared_error(y_true, preds))\n",
        "\n",
        "        results.append((mae, rmse))\n",
        "\n",
        "    return results\n",
        "\n",
        "transformer_results = rolling_origin_cv_transformer(X_scaled, y_scaled)\n",
        "print(\"Transformer CV Results:\", transformer_results)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qWn09XgUpAZV",
        "outputId": "dbf602d0-2938-4a33-c15a-ef0e0772ac28"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformer CV Results: [(11.744543744136015, np.float64(13.601048811259064)), (9.364332898901756, np.float64(11.799676912851364)), (7.42360746175388, np.float64(9.503335387847013)), (8.592534488631792, np.float64(10.540018246267659)), (9.212344548916594, np.float64(11.006044862304996))]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def rolling_origin_cv_xgb(X, y, n_splits=5, train_size=800, step=100):\n",
        "\n",
        "    results = []\n",
        "    X_flat = X.reshape(X.shape[0], -1)\n",
        "\n",
        "    for i in range(n_splits):\n",
        "\n",
        "        end_train = train_size + i * step\n",
        "        end_test = end_train + step\n",
        "\n",
        "        X_train = X_flat[:end_train]\n",
        "        y_train = y[:end_train]\n",
        "\n",
        "        X_test = X_flat[end_train:end_test]\n",
        "        y_test = y[end_train:end_test]\n",
        "\n",
        "        model = XGBRegressor(n_estimators=200, max_depth=4)\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "        preds = model.predict(X_test)\n",
        "\n",
        "        preds = scaler_y.inverse_transform(preds.reshape(-1,1))\n",
        "        y_true = scaler_y.inverse_transform(y_test.reshape(-1,1))\n",
        "\n",
        "        mae = mean_absolute_error(y_true, preds)\n",
        "        rmse = np.sqrt(mean_squared_error(y_true, preds))\n",
        "\n",
        "        results.append((mae, rmse))\n",
        "\n",
        "    return results\n",
        "\n",
        "xgb_results = rolling_origin_cv_xgb(X_scaled, y_scaled)\n",
        "print(\"XGBoost CV Results:\", xgb_results)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LKKqFxzlpD2E",
        "outputId": "ecdf2b27-a104-4627-c643-f8a80aebc5fa"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost CV Results: [(9.170314282781543, np.float64(11.308523246329775)), (7.585778688365378, np.float64(9.539443328573423)), (8.305801233528937, np.float64(10.012052895221965)), (7.078967464919645, np.float64(8.79418290369048)), (11.218591926931074, np.float64(13.344101172959405))]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def average_results(results):\n",
        "    maes = [r[0] for r in results]\n",
        "    rmses = [r[1] for r in results]\n",
        "    return np.mean(maes), np.mean(rmses)\n",
        "\n",
        "print(\"Transformer Avg:\", average_results(transformer_results))\n",
        "print(\"XGBoost Avg:\", average_results(xgb_results))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZWB2iCDXpMw_",
        "outputId": "9a5ddc60-366b-4d3b-a0dd-e1c72d3cbedf"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformer Avg: (np.float64(9.267472628468008), np.float64(11.29002484410602))\n",
            "XGBoost Avg: (np.float64(8.671890719305315), np.float64(10.599660709355009))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Project Explanation\n",
        "1. Transformer Architecture\n",
        "This project uses a Transformer Encoder model instead of an LSTM-based model.\n",
        "The Transformer leverages multi-head self-attention mechanisms, allowing the model to capture long-range temporal dependencies more effectively.\n",
        "Unlike recurrent models, the Transformer processes sequences in parallel and learns contextual importance through attention weights.\n",
        "Model Configuration:\n",
        "d_model = 64 (embedding dimension)\n",
        "nhead = 4 (multi-head attention)\n",
        "num_layers = 2 (stacked encoder layers)\n",
        "learning_rate = 0.001 (Adam optimizer)\n",
        "These values were chosen to balance representational capacity and computational efficiency.\n",
        "2. XGBoost Baseline\n",
        "To provide a strong benchmark, XGBoost regression was implemented using lagged time-series features.\n",
        "Hyperparameters:\n",
        "n_estimators = 200\n",
        "max_depth = 4\n",
        "XGBoost serves as a powerful tree-based ensemble baseline, commonly used in structured time-series forecasting tasks.\n",
        "3. Rolling-Origin Cross-Validation\n",
        "Instead of a simple train-test split, rolling-origin cross-validation was implemented.\n",
        "Procedure:\n",
        "Initial training window = 800 observations\n",
        "Test window = 100 observations\n",
        "Window expands forward in each fold\n",
        "This simulates real-world forecasting scenarios and prevents data leakage.\n",
        "Evaluation metrics:\n",
        "MAE (Mean Absolute Error)\n",
        "RMSE (Root Mean Squared Error)\n",
        "Performance is averaged across all folds.\n",
        "4. Data Complexity\n",
        "The synthetic dataset includes:\n",
        "Linear trend\n",
        "Seasonality\n",
        "Regime shift after time step 800\n",
        "Heteroscedastic noise (variance increases over time)\n",
        "This ensures the forecasting task reflects realistic time-series challenges."
      ],
      "metadata": {
        "id": "tTTyFk5PpwYC"
      }
    }
  ]
}